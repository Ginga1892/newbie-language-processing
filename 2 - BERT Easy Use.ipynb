{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三步教你用BERT\n",
    "问题：这才第二篇就开始BERT了？？\n",
    "\n",
    "是的，有一说一，会读写数据之后确实可以用BERT。因为BERT该做的都帮你做好了。。。\n",
    "\n",
    "接下来先简单夸一下BERT：\n",
    "\n",
    "BERT可以搞定很多nlp任务，包括推理、情感分析、问答。虽然距离BERT提出已经快两年了，但目前很多项目仍旧会先用它跑，大部分任务用BERT不优化的情况下精度都能上90%。没错BERT就是如此强大！尽管目前有性能更好的XL-Net、SemBERT等等，依旧无法抵挡BERT的存在。\n",
    "\n",
    "接下来我们来下载谷歌官方的原版[BERT](https://github.com/google-research/bert)。\n",
    "\n",
    "然后你会看到很多关于BERT模型的介绍，感兴趣可以戳[原文](https://arxiv.org/pdf/1810.04805.pdf)。\n",
    "\n",
    "下面以我浅薄的认识给大家概括一下BERT是啥。\n",
    "\n",
    "BERT分为两部分，pre-train和fine-tune。pre-train就是先用大量的语料训练模型，得到预训练的embedding。fine-tune就是以预训练为基础，在下游任务上再次训练embedding。一般我们用BERT都是用它预训练好的模型，在上面跑fine-tune。所以这篇的完整题目应该叫做：教你基于BERT预训练来fine-tune。\n",
    "\n",
    "BERT预训练部分有两个任务：\n",
    "* 完形填空：简单来说就是把文章里一些词用特殊标记覆盖掉，然后让模型预测这个地方的词\n",
    "* 预测下一句：由于要适应推理方面的任务，光训练词还不够，因此BERT预训练的时候还需要根据前一句话去预测下一句\n",
    "\n",
    "整个BERT都基于名为[Transformer](https://arxiv.org/pdf/1706.03762.pdf)的编解码模型，整个模型都采用self-attention，达到并行编码的目的。（问题：解码能不能并行？等我再去看看源码？）\n",
    "\n",
    "好了终于扯完了，接下里开始教你用BERT来fine-tune！\n",
    "\n",
    "三步：\n",
    "* 配置环境，下载预训练模型\n",
    "* 在run_classifier.py添加任务进程\n",
    "* 跑模型\n",
    "\n",
    "## 第一步\n",
    "首先你要启一个Tensorflow的环境，cpu或者gpu都行，但要保证环境里面tensorflow-xxx的版本都一致，否则会报错。\n",
    "\n",
    "下载预训练模型，比如[中文](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)。\n",
    "\n",
    "里面模型还挺多的，练手的话可以先用base，large的模型很大，cased/uncased是大小写区不区分，如果你是中文的话就只有一个选择。\n",
    "\n",
    "下载好之后解压，你会看一个`.ckpt`文件，这个就是我们要用的模型。另外vocab是词表，可以打开看一下里面都有哪些语料，包括一些特殊标记。然后把`chinese_L-12_H-768_A-12`这个文件夹放到全是代码的那个目录里。\n",
    "\n",
    "## 第二步\n",
    "这步稍微复杂点，需要修改代码。\n",
    "\n",
    "首先你需要知道BERT能跑哪些任务，虽然实际上它都能跑。。。只要你会用。这里先介绍一下最简单的sentence/sentence-pair的分类任务，也是谷歌网页上给出的。顾名思义就是一句话或者两句话，多句话你可以把它们拼接起来，随后有个分类标签，也就是：\n",
    "* text_a\n",
    "* text_b（可无）\n",
    "* label\n",
    "\n",
    "知道这个之后，我们打开`run_classifier.py`，找到第783行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    processors = {\n",
    "      \"cola\": ColaProcessor,\n",
    "      \"mnli\": MnliProcessor,\n",
    "      \"mrpc\": MrpcProcessor,\n",
    "      \"xnli\": XnliProcessor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有个processors，这四个是它默认可选的任务，乍一看确实是句子对类型的任务。\n",
    "\n",
    "我们在里面添加一个自己的进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    processors = {\n",
    "      \"cola\": ColaProcessor,\n",
    "      \"mnli\": MnliProcessor,\n",
    "      \"mrpc\": MrpcProcessor,\n",
    "      \"xnli\": XnliProcessor,\n",
    "      \"xxxx\": XxxxProcessor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加完名字之后，你得告诉BERT这个具体是什么任务，代码网上翻，找到177行。\n",
    "\n",
    "这里开始有很多个class，可以发现最上面的DataProcessor是基类，下面的类都是它的实现，且名字与刚刚的processors完全对应。\n",
    "\n",
    "所以我们也要写一个类似的继承类。（其实很简单，因为这个类的作用就是告诉BERT我们的数据长什么样）\n",
    "\n",
    "首先我们先随便复制一个其中的类，名字先改成XxxxProcessor，这里复制的是MnliProcessor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XxxxProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir): \n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "          self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "          self._read_tsv(os.path.join(data_dir, \"dev_matched.tsv\")), \"dev_matched\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "          self._read_tsv(os.path.join(data_dir, \"test_matched.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, tokenization.convert_to_unicode(line[0]))\n",
    "            text_a = tokenization.convert_to_unicode(line[8])\n",
    "            text_b = tokenization.convert_to_unicode(line[9])\n",
    "            if set_type == \"test\":\n",
    "                label = \"contradiction\"\n",
    "            else:\n",
    "                label = tokenization.convert_to_unicode(line[-1])\n",
    "            examples.append(\n",
    "              InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以看到前三个函数是分别获取训练集、验证集和测试集，并且默认格式是tsv。如果你想用csv或者别的当然也可以，那就修改一下前面的read_tsv函数，在197行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_tsv(cls, input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "        lines = []\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现这个函数其实很简单，结合上一篇的内容（最基础部分），它就是把tsv文件的每一行加入到lines数组里。\n",
    "\n",
    "注意：这里是tsv，如果你想用csv就改一下reader的参数，甚至你想用json都可以改。这里为了方便就用tsv，代码可以完全不用改。你只要把你的数据集都变成tsv格式，名字和代码里对应就行，这一步可以先放一放。\n",
    "\n",
    "接下来看到get_labels函数，就是告诉模型你的标签有几种。现在假设我们的任务是一个简单二分类，标签0和1，标签是文字也是可以的，但必须是字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(self):\n",
    "    \"\"\"See base class.\"\"\"\n",
    "    return [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后就是create_examples函数，可以发现参数里的lines就是read_tsv返回的lines。i=0的时候continue是因为第一行是表头。\n",
    "\n",
    "接下来就是根据你的数据集来写了，假设我们的tsv文件长这样：\n",
    "```tsv\n",
    "A B Label\n",
    "I am a boy. I am not a girl. 0\n",
    "I am 60 years old. I am a boy. 1\n",
    "```\n",
    "那么我们的text_a就是line\\[0\\], text_b就是line\\[1\\]，label就是line\\[-1\\]。guid是id，可以不管。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_examples(self, lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        guid = \"%s-%s\" % (set_type, tokenization.convert_to_unicode(line[-1]))\n",
    "        text_a = tokenization.convert_to_unicode(line[0])\n",
    "        text_b = tokenization.convert_to_unicode(line[1])\n",
    "        label = tokenization.convert_to_unicode(line[-1])\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "没错，就是这样！\n",
    "\n",
    "最后我们的训练样本会通过IntputExample送到模型里。\n",
    "\n",
    "代码就这样修改完了，其实整个添加的过程很随意，text_a和text_b根据你实际的数据集来就行。如果你代码玩得溜，你可以随便怎么改。\n",
    "\n",
    "## 第三步\n",
    "**训练**\n",
    "```bash\n",
    "export BERT_BASE_DIR=chinese_L-12_H-768_A-12\n",
    "export GLUE_DIR=data\n",
    "export OUTPUT_DIR=output\n",
    "```\n",
    "BERT_BASE_DIR是告诉BERT预训练文件在哪，之前已经放到同一个目录里。GLUE_DIR是数据集的路径，需要自己新建。我这里新建了一个`data`文件夹，记得把你自己的`train.tsv`/`dev_matched.tsv`/`test_matched.tsv`三个文件放进去。最好再指定一个输出路径OUTPUT_DIR存放结果，后面还会用到。\n",
    "\n",
    "然后就能训练了。\n",
    "```bash\n",
    "python run_classifier.py \\\n",
    "  --task_name=XXXX \\\n",
    "  --do_train=true \\\n",
    "  --do_eval=true \\\n",
    "  --data_dir=$GLUE_DIR/ \\\n",
    "  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n",
    "  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n",
    "  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\\n",
    "  --max_seq_length=32 \\\n",
    "  --train_batch_size=128 \\\n",
    "  --learning_rate=2e-5 \\\n",
    "  --num_train_epochs=3.0 \\\n",
    "  --output_dir=$OUTPUT_DIR/\n",
    "  ```\n",
    " task_name注意改成自己的名字，如果你的内存比较小可以减小train_batch_size，如果你的句子比较短也可以减小max_seq_length。\n",
    " \n",
    " **预测**\n",
    " \n",
    "2000 years later...\n",
    "\n",
    "训练好的模型会存在output里，同样是ckpt文件，另外还能找到一个eval的文件，里面会告诉你验证集的精度和损失等等。\n",
    "\n",
    "这里当然要用我们刚跑出来的模型作为分类器。\n",
    "```bash\n",
    "export TRAINED_CLASSIFIER=output\n",
    "```\n",
    "开始预测！这里同样输出到output里。\n",
    "```bash\n",
    "python run_classifier.py \\\n",
    "  --task_name=XXXX \\\n",
    "  --do_predict=true \\\n",
    "  --data_dir=$GLUE_DIR/ \\\n",
    "  --vocab_file=$BERT_BASE_DIR/vocab.txt \\\n",
    "  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\\n",
    "  --init_checkpoint=$TRAINED_CLASSIFIER \\\n",
    "  --max_seq_length=128 \\\n",
    "  --output_dir=$OUTPUT_DIR/\n",
    " ```\n",
    " **结果**\n",
    " \n",
    " 预测很快就能跑完，结果在output里一个叫`test_results.tsv`的文件。打开之后是一群浮点数，分别对应每个标签的概率。比如我的输出应该长这样：\n",
    " ```tsv\n",
    " 0.9432 0.0568\n",
    " 0.0463 0.9537\n",
    " ```\n",
    " 你可以简单写个程序把它们转换成你要的标签~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLabel(file_name):\n",
    "    labels = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        file = csv.reader(f, delimiter='\\t')\n",
    "        for line in file:\n",
    "            label = 0 if line[0] > line[1] else 1\n",
    "            labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三步跑BERT就这样搞定了，是不是很快！\n",
    "\n",
    "不过BERT并没有这么简单，还有很多用法，比如BERT+xxx、xxxBERT。这里只是入门级的用法，要想用得好得研究一下源码和原文。\n",
    "\n",
    "感谢收看！\n",
    "\n",
    "## 下回：NLP前的数据预处理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bs-env]",
   "language": "python",
   "name": "conda-env-bs-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
